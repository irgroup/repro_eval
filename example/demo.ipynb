{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "intro.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dy3GpaAVtDJ"
      },
      "source": [
        "# An Introduction to `repro_eval`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vP09TvRbZnI"
      },
      "source": [
        "This notebook introduces the functionalities of `repro_eval`. We provide sample data that has to be downloaded in advance, but it is also possible to upload your runs and evaluate the reproducibilty of your experiments with this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HvZTRuDb0FC"
      },
      "source": [
        "#### Install `repro_eval` from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Odv7-WVt4o"
      },
      "source": [
        "!pip install git+https://github.com/irgroup/repro_eval.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUkSqYmgb4yD"
      },
      "source": [
        "#### Download the sample data and extract it\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw2nFqDZWRyP"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ncu49e91mosidei/data.tar.gz\n",
        "!tar -xzvf ./data.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SN1XavoVtDL"
      },
      "source": [
        "### Imports\n",
        "\n",
        "Once installed, the Evaluator classes for the evaluation of reproducibility and replicability can be imported. In this notebook, we also include other Python packages that are not necessarily required when using `repro_eval` for your experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCVSY0rGVtDM"
      },
      "source": [
        "from repro_eval.Evaluator import RpdEvaluator, RplEvaluator\n",
        "from repro_eval.util import arp, arp_scores, print_base_adv, print_simple_line, trim\n",
        "\n",
        "import pytrec_eval\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "sns.set_style('whitegrid')\n",
        "colors = sns.color_palette()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRZlhiToVtDS"
      },
      "source": [
        "### Path definition\n",
        "You can modify these paths and adapt them to your experiments. The entire notebook should be usable with your experiments when they comply with the given evaluation scenario. First, we need two kind of runs - a baseline run and an advanced run (that outperforms the baseline run). Second, for the evaluation of replicability, the replicated runs should be derived from another target collection. The dictionaries `runs_rpd` and `runs_rpl` contain runs with different parametrizations, but it should also be possible to include just one version for both the baseline and advanced run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyFqKV8KVtDT"
      },
      "source": [
        "QREL = './data/qrels/core17.txt'\n",
        "QREL_RPL = './data/qrels/core18.txt'\n",
        "ORIG_B = './data/runs/orig/input.WCrobust04'\n",
        "ORIG_A = './data/runs/orig/input.WCrobust0405'\n",
        "RPD_B = './data/runs/rpd/14/irc_task1_WCrobust04_001'\n",
        "RPD_A = './data/runs/rpd/14/irc_task1_WCrobust0405_001'\n",
        "RPL_B = './data/runs/rpl/14/irc_task2_WCrobust04_001'\n",
        "RPL_A = './data/runs/rpl/14/irc_task2_WCrobust0405_001'\n",
        "MEASURE = 'ndcg'\n",
        "\n",
        "runs_rpd = {\n",
        "    'rpd_wcr04_tf_1':\n",
        "        {'path': './data/runs/rpd/45/irc_task1_WCrobust04_001'},\n",
        "    'rpd_wcr0405_tf_1':\n",
        "        {'path': './data/runs/rpd/45/irc_task1_WCrobust0405_001'},\n",
        "    'rpd_wcr04_tf_2':\n",
        "        {'path': './data/runs/rpd/46/irc_task1_WCrobust04_001'},\n",
        "    'rpd_wcr0405_tf_2':\n",
        "        {'path': './data/runs/rpd/46/irc_task1_WCrobust0405_001'},\n",
        "    'rpd_wcr04_tf_3':\n",
        "        {'path': './data/runs/rpd/47/irc_task1_WCrobust04_001'},\n",
        "    'rpd_wcr0405_tf_3':\n",
        "        {'path': './data/runs/rpd/47/irc_task1_WCrobust0405_001'},\n",
        "    'rpd_wcr04_tf_4':\n",
        "        {'path': './data/runs/rpd/48/irc_task1_WCrobust04_001'},\n",
        "    'rpd_wcr0405_tf_4':\n",
        "        {'path': './data/runs/rpd/48/irc_task1_WCrobust0405_001'},\n",
        "    'rpd_wcr04_tf_5':\n",
        "        {'path': './data/runs/rpd/49/irc_task1_WCrobust04_001'},\n",
        "    'rpd_wcr0405_tf_5':\n",
        "        {'path': './data/runs/rpd/49/irc_task1_WCrobust0405_001'}\n",
        "}\n",
        "\n",
        "runs_rpl = {\n",
        "    'rpl_wcr04_tf_1':\n",
        "        {'path': './data/runs/rpl/45/irc_task2_WCrobust04_001'},\n",
        "    'rpl_wcr0405_tf_1':\n",
        "        {'path': './data/runs/rpl/45/irc_task2_WCrobust0405_001'},\n",
        "    'rpl_wcr04_tf_2':\n",
        "        {'path': './data/runs/rpl/46/irc_task2_WCrobust04_001'},\n",
        "    'rpl_wcr0405_tf_2':\n",
        "        {'path': './data/runs/rpl/46/irc_task2_WCrobust0405_001'},\n",
        "    'rpl_wcr04_tf_3':\n",
        "        {'path': './data/runs/rpl/47/irc_task2_WCrobust04_001'},\n",
        "    'rpl_wcr0405_tf_3':\n",
        "        {'path': './data/runs/rpl/47/irc_task2_WCrobust0405_001'},\n",
        "    'rpl_wcr04_tf_4':\n",
        "        {'path': './data/runs/rpl/48/irc_task2_WCrobust04_001'},\n",
        "    'rpl_wcr0405_tf_4':\n",
        "        {'path': './data/runs/rpl/48/irc_task2_WCrobust0405_001'},\n",
        "    'rpl_wcr04_tf_5':\n",
        "        {'path': './data/runs/rpl/49/irc_task2_WCrobust04_001'},\n",
        "    'rpl_wcr0405_tf_5':\n",
        "        {'path': './data/runs/rpl/49/irc_task2_WCrobust0405_001'}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5YPFEL5VtDa"
      },
      "source": [
        "Define a helping function for plotting the average retrieval performance (ARP) later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AdKB2-VtDb"
      },
      "source": [
        "def average_retrieval_performance(baseline_scores, reproduced_scores: dict, measures: list, xlabel: str, ylabel: str):\n",
        "    reproduced_scores_arp = [arp_scores(topic_scores) for idx, topic_scores in reproduced_scores.items()]\n",
        "    baseline_scores_arp = arp_scores(baseline_scores)\n",
        "    index = list(reproduced_scores.keys())\n",
        "    df_content = {}\n",
        "    for measure in measures:\n",
        "        df_content[measure] = [scores.get(measure) for scores in reproduced_scores_arp]\n",
        "    df = pd.DataFrame(df_content, index=index)\n",
        "\n",
        "    plt.figure()\n",
        "    ax = df.plot.bar(rot=0, figsize=(10, 6))\n",
        "    for num, measure in enumerate(measures):\n",
        "        orig_val = baseline_scores_arp.get(measure)\n",
        "        ax.hlines(orig_val, -.5, 5.5, linestyles='dashed', color=colors[num])\n",
        "        ax.annotate(' ', (num, orig_val), color=colors[num])\n",
        "        ax.set_ylim(0.0, 1.0)\n",
        "\n",
        "    legend_content = [measure + ' (orig)' for measure in measures] + [measure + ' (rpl)' for measure in measures]\n",
        "    ax.legend(legend_content, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vn-UoX3VtDg"
      },
      "source": [
        "### Evaluating Reproducibility\n",
        "The following code snippet instantiates a reproducibility evaluator `RpdEvaluator` and determines Kendall's tau Union (KTU), the Rank-biased Overlap (RBO), the Root-Mean-Square-Error (RMSE), the Effect Ratio (ER), the Delta Relative Improvement (DRI) and the p-values of the paired t-test. Please be aware, that it takes some time for the RBO to be computed. We've included a progress bar to give you some feedback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFybABY7VtDh"
      },
      "source": [
        "rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=RPD_B,\n",
        "                        run_a_rep_path=RPD_A)\n",
        "\n",
        "rpd_eval.trim()\n",
        "rpd_eval.evaluate()\n",
        "\n",
        "# KTU\n",
        "ktau = rpd_eval.ktau_union()\n",
        "print(\"Kendall's tau Union (KTU)\")\n",
        "print('------------------------------------------------------------------')\n",
        "for topic, value in ktau.get('baseline').items():\n",
        "    print_base_adv(topic, 'KTU', value, ktau.get('advanced').get(topic))\n",
        "print_base_adv('ARP', 'KTU', arp(ktau.get('baseline')), arp(ktau.get('advanced')))\n",
        "\n",
        "# RBO\n",
        "rbo = rpd_eval.rbo(print_feedback=True)\n",
        "print(\"Rank-biased Overlap (RBO)\")\n",
        "print('------------------------------------------------------------------')\n",
        "for topic, value in rbo.get('baseline').items():\n",
        "    print_base_adv(topic, 'RBO', value, rbo.get('advanced').get(topic))\n",
        "print_base_adv('ARP', 'RBO', arp(rbo.get('baseline')), arp(rbo.get('advanced')))\n",
        "\n",
        "# RMSE\n",
        "rmse = rpd_eval.rmse()\n",
        "print(\"Root mean square error (RMSE)\")\n",
        "print('------------------------------------------------------------------')\n",
        "for measure, value in rmse.get('baseline').items():\n",
        "    print_base_adv(measure, 'RMSE', value, rmse.get('advanced').get(measure))\n",
        "\n",
        "# ER\n",
        "print(\"Effect ratio (ER)\")\n",
        "print('------------------------------------------------------------------')\n",
        "er = rpd_eval.er()\n",
        "for measure, value in er.items():\n",
        "    print_simple_line(measure, 'ER', value)\n",
        "\n",
        "# DRI\n",
        "print(\"Delta Relative Improvement (DRI)\")\n",
        "print('------------------------------------------------------------------')\n",
        "dri = rpd_eval.dri()\n",
        "for measure, value in dri.items():\n",
        "    print_simple_line(measure, 'DRI', value)\n",
        "\n",
        "# ttest\n",
        "pvals = rpd_eval.ttest()\n",
        "print(\"Two-tailed paired t-test (p-value)\")\n",
        "print('------------------------------------------------------------------')\n",
        "for measure, value in pvals.get('baseline').items():\n",
        "    print_base_adv(measure, 'PVAL', value, pvals.get('advanced').get(measure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9acgVo-CVtDm"
      },
      "source": [
        "### Comparing the Average Retrieval Performance (ARP) of different parametrizations \n",
        "The following code snippet determines the ARP scores and compares them via a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtASw_fMVtDn"
      },
      "source": [
        "rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=None,\n",
        "                        run_a_rep_path=None)\n",
        "\n",
        "rpd_eval.trim()\n",
        "rpd_eval.evaluate()\n",
        "\n",
        "for run_name, info in runs_rpd.items():\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        trim(info['run'])\n",
        "        info['scores'] = rpd_eval.evaluate(info['run'])\n",
        "\n",
        "average_retrieval_performance(rpd_eval.run_b_orig_score,\n",
        "                              {\n",
        "                                  'tf_1': runs_rpd.get('rpd_wcr04_tf_1').get('scores'),\n",
        "                                  'tf_2': runs_rpd.get('rpd_wcr04_tf_2').get('scores'),\n",
        "                                  'tf_3': runs_rpd.get('rpd_wcr04_tf_3').get('scores'),\n",
        "                                  'tf_4': runs_rpd.get('rpd_wcr04_tf_4').get('scores'),\n",
        "                                  'tf_5': runs_rpd.get('rpd_wcr04_tf_5').get('scores'),\n",
        "                              },\n",
        "                              measures=['P_10', 'ndcg', 'bpref', 'map'],\n",
        "                              xlabel='Reproduced run (wcr04)',\n",
        "                              ylabel='Score')\n",
        "\n",
        "average_retrieval_performance(rpd_eval.run_a_orig_score,\n",
        "                              {\n",
        "                                  'tf_1': runs_rpd.get('rpd_wcr0405_tf_1').get('scores'),\n",
        "                                  'tf_2': runs_rpd.get('rpd_wcr0405_tf_2').get('scores'),\n",
        "                                  'tf_3': runs_rpd.get('rpd_wcr0405_tf_3').get('scores'),\n",
        "                                  'tf_4': runs_rpd.get('rpd_wcr0405_tf_4').get('scores'),\n",
        "                                  'tf_5': runs_rpd.get('rpd_wcr0405_tf_5').get('scores'),\n",
        "                              },\n",
        "                              measures=['P_10', 'ndcg', 'bpref', 'map'],\n",
        "                              xlabel='Reproduced run (wcr0405)',\n",
        "                              ylabel='Score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQVD43jBVtDs"
      },
      "source": [
        "### Kendall's tau Union (KTU) across different cut-offs\n",
        "The following code snippet compares the ordering of documents for the reproduced runs across different cut-off ranks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8COriFZVtDt"
      },
      "source": [
        "cutoffs = [1000, 100, 50, 20, 10, 5]\n",
        "\n",
        "# BASELINE\n",
        "for run_name, info in zip(list(runs_rpd.keys())[::2], list(runs_rpd.values())[::2]):\n",
        "    rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                            run_b_orig_path=ORIG_B,\n",
        "                            run_a_orig_path=ORIG_A,\n",
        "                            run_b_rep_path=None,\n",
        "                            run_a_rep_path=None)\n",
        "\n",
        "    rpd_eval.trim()\n",
        "    rpd_eval.evaluate()\n",
        "\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        for cutoff in cutoffs:\n",
        "            rpd_eval.trim(cutoff)\n",
        "            rpd_eval.trim(cutoff, info['run'])\n",
        "            info['ktu_' + str(cutoff)] = arp(rpd_eval.ktau_union(info['run'])['baseline'])\n",
        "\n",
        "df_content = {}\n",
        "for run_name, info in zip(list(runs_rpd.keys())[::2], list(runs_rpd.values())[::2]):\n",
        "    df_content[run_name] = [info.get('ktu_' + str(cutoff)) for cutoff in cutoffs[::-1]]\n",
        "\n",
        "plt.figure()\n",
        "ax = pd.DataFrame(data=df_content, index=[str(cutoff) for cutoff in cutoffs[::-1]]).plot(style='o-', figsize=(10, 6))\n",
        "ax.set_xlabel('Cut-off values')\n",
        "ax.set_ylabel(r\"Kendall's $\\tau$\")\n",
        "plt.show()\n",
        "\n",
        "# ADVANCED\n",
        "for run_name, info in zip(list(runs_rpd.keys())[1::2], list(runs_rpd.values())[1::2]):\n",
        "    rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                            run_b_orig_path=ORIG_B,\n",
        "                            run_a_orig_path=ORIG_A,\n",
        "                            run_b_rep_path=None,\n",
        "                            run_a_rep_path=None)\n",
        "\n",
        "    rpd_eval.trim()\n",
        "    rpd_eval.evaluate()\n",
        "\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        for cutoff in cutoffs:\n",
        "            rpd_eval.trim(cutoff)\n",
        "            rpd_eval.trim(cutoff, info['run'])\n",
        "            # scores = rpl_eval.evaluate(info['run'])\n",
        "            info['ktu_' + str(cutoff)] = arp(rpd_eval.ktau_union(info['run'])['baseline'])\n",
        "\n",
        "df_content = {}\n",
        "for run_name, info in zip(list(runs_rpd.keys())[1::2], list(runs_rpd.values())[1::2]):\n",
        "    df_content[run_name] = [info.get('ktu_' + str(cutoff)) for cutoff in cutoffs[::-1]]\n",
        "\n",
        "plt.figure()\n",
        "ax = pd.DataFrame(data=df_content, index=[str(cutoff) for cutoff in cutoffs[::-1]]).plot(style='o-', figsize=(10, 6))\n",
        "ax.set_xlabel('Cut-off values')\n",
        "ax.set_ylabel(r\"Kendall's $\\tau$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Prc5aqVtDz"
      },
      "source": [
        "## Root-Mean-Square-Error (RMSE) across different cut-offs\n",
        "The following code snippet compares the reproduced runs at the level of effectiveness by determining the RMSE across different cut-off ranks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2hsEkeGVtD0"
      },
      "source": [
        "rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=None,\n",
        "                        run_a_rep_path=None)\n",
        "\n",
        "rpd_eval.trim()\n",
        "rpd_eval.evaluate()\n",
        "\n",
        "for run_name, info in runs_rpd.items():\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        trim(info['run'])\n",
        "        info['scores'] = rpd_eval.evaluate(info['run'])\n",
        "        info['rmse'] = rpd_eval.rmse(run_b_score=info['scores'])\n",
        "\n",
        "\n",
        "baseline_runs = ['rpd_wcr04_tf_1', 'rpd_wcr04_tf_2', 'rpd_wcr04_tf_3', 'rpd_wcr04_tf_4', 'rpd_wcr04_tf_5']\n",
        "advanced_runs = ['rpd_wcr0405_tf_1', 'rpd_wcr0405_tf_2', 'rpd_wcr0405_tf_3', 'rpd_wcr0405_tf_4', 'rpd_wcr0405_tf_5']\n",
        "cutoffs = ['5', '10', '15', '20', '30', '100', '200', '500', '1000']\n",
        "\n",
        "df_content = {}\n",
        "for run_name in baseline_runs:\n",
        "    df_content[run_name] = [runs_rpd[run_name]['rmse']['baseline']['ndcg_cut_' + co] for co in cutoffs]\n",
        "\n",
        "df = pd.DataFrame(df_content, index=cutoffs)\n",
        "plt.figure()\n",
        "ax = df.plot.line(style='o-', figsize=(10, 6))\n",
        "ax.set_xlabel('Cut-off values')\n",
        "ax.set_ylabel('RMSE')\n",
        "plt.show()\n",
        "\n",
        "df_content = {}\n",
        "for run_name in advanced_runs:\n",
        "    df_content[run_name] = [runs_rpd[run_name]['rmse']['baseline']['ndcg_cut_' + co] for co in cutoffs]\n",
        "\n",
        "df = pd.DataFrame(df_content, index=cutoffs)\n",
        "plt.figure()\n",
        "ax = df.plot.line(style='o-', figsize=(10, 6))\n",
        "ax.set_xlabel('Cut-off values')\n",
        "ax.set_ylabel('RMSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKj4uBanVtD5"
      },
      "source": [
        "## Exploring the space of reproducibility at the level of overall effects\n",
        "The following code snippet plots the Delta Relative Improvement (DRI) against the Effect Ratio (ER). Having runs with different parametrizations at hand, we can compare them in the cartesian plane. As a rule of thumb, we can say the closer a point to (ER 1, DRI 0), the better the reproduction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scmfh0ZfVtD5"
      },
      "source": [
        "rpd_eval = RpdEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=None,\n",
        "                        run_a_rep_path=None)\n",
        "\n",
        "rpd_eval.trim()\n",
        "rpd_eval.evaluate()\n",
        "\n",
        "for run_name, info in runs_rpd.items():\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        trim(info['run'])\n",
        "        info['scores'] = rpd_eval.evaluate(info['run'])\n",
        "\n",
        "dri_er = {\n",
        "    'wcr_tf_1': {\n",
        "        'er': rpd_eval.er(runs_rpd['rpd_wcr04_tf_1']['scores'], runs_rpd['rpd_wcr0405_tf_1']['scores']),\n",
        "        'dri': rpd_eval.dri(runs_rpd['rpd_wcr04_tf_1']['scores'], runs_rpd['rpd_wcr0405_tf_1']['scores'])\n",
        "    },\n",
        "    'wcr_tf_2': {\n",
        "        'er': rpd_eval.er(runs_rpd['rpd_wcr04_tf_2']['scores'], runs_rpd['rpd_wcr0405_tf_2']['scores']),\n",
        "        'dri': rpd_eval.dri(runs_rpd['rpd_wcr04_tf_2']['scores'], runs_rpd['rpd_wcr0405_tf_2']['scores'])\n",
        "    },\n",
        "    'wcr_tf_3': {\n",
        "        'er': rpd_eval.er(runs_rpd['rpd_wcr04_tf_3']['scores'], runs_rpd['rpd_wcr0405_tf_3']['scores']),\n",
        "        'dri': rpd_eval.dri(runs_rpd['rpd_wcr04_tf_3']['scores'], runs_rpd['rpd_wcr0405_tf_3']['scores'])\n",
        "    },\n",
        "    'wcr_tf_4': {\n",
        "        'er': rpd_eval.er(runs_rpd['rpd_wcr04_tf_4']['scores'], runs_rpd['rpd_wcr0405_tf_4']['scores']),\n",
        "        'dri': rpd_eval.dri(runs_rpd['rpd_wcr04_tf_4']['scores'], runs_rpd['rpd_wcr0405_tf_4']['scores'])\n",
        "    },\n",
        "    'wcr_tf_5': {\n",
        "        'er': rpd_eval.er(runs_rpd['rpd_wcr04_tf_5']['scores'], runs_rpd['rpd_wcr0405_tf_5']['scores']),\n",
        "        'dri': rpd_eval.dri(runs_rpd['rpd_wcr04_tf_5']['scores'], runs_rpd['rpd_wcr0405_tf_5']['scores'])\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "measures = ['P_10', 'map', 'ndcg']\n",
        "marker_color = [('o', 'b'), ('^', 'g'), ('v', 'r')]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
        "ax1.set_xlabel('Effect Ratio (ER)')\n",
        "ax1.set_ylabel(u'Delta Relative Improvement (ΔRI)')\n",
        "\n",
        "for measure, mk in zip(measures, marker_color):\n",
        "    ax1.plot([dri_er[r]['er'][measure] for r in dri_er.keys()],\n",
        "             [dri_er[r]['dri'][measure] for r in dri_er.keys()],\n",
        "             marker=mk[0], color=mk[1], linestyle='None', label=measure)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='k')\n",
        "fig.tight_layout()\n",
        "plt.axhline(0, color='grey')\n",
        "plt.axvline(1, color='grey')\n",
        "plt.legend()\n",
        "plt.title('Reproducibility')\n",
        "\n",
        "for m in measures:\n",
        "  for r in dri_er.keys():\n",
        "    plt.text(x = dri_er[r]['er'][m], \n",
        "             y = dri_er[r]['dri'][m],\n",
        "             s = r) \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGck2XO0VtD9"
      },
      "source": [
        "## Evaluating Replicability\n",
        "The following code snippet instantiates a replicability evaluator `RplEvaluator` and determines the Effect Ratio (ER), the Delta Relative Improvement (DRI) and the p-values of the unpaired t-test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz8ExjJOVtD-"
      },
      "source": [
        "rpl_eval = RplEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=RPL_B,\n",
        "                        run_a_rep_path=RPL_A,\n",
        "                        qrel_rpl_path=QREL_RPL)\n",
        "\n",
        "rpl_eval.trim()\n",
        "rpl_eval.evaluate()\n",
        "\n",
        "# ER\n",
        "print(\"Effect ratio (ER)\")\n",
        "print('------------------------------------------------------------------')\n",
        "er = rpl_eval.er()\n",
        "for measure, value in er.items():\n",
        "    print_simple_line(measure, 'ER', value)\n",
        "\n",
        "# DRI\n",
        "print(\"Delta Relative Improvement (DRI)\")\n",
        "print('------------------------------------------------------------------')\n",
        "dri = rpl_eval.dri()\n",
        "for measure, value in dri.items():\n",
        "    print_simple_line(measure, 'DRI', value)\n",
        "\n",
        "# ttest\n",
        "pvals = rpl_eval.ttest()\n",
        "print(\"Two-tailed unpaired t-test (p-value)\")\n",
        "print('------------------------------------------------------------------')\n",
        "for measure, value in pvals.get('baseline').items():\n",
        "    print_base_adv(measure, 'PVAL', value, pvals.get('advanced').get(measure))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU4QlxI0VtEC"
      },
      "source": [
        "## Exploring the space of replicability at the level of overall effects\n",
        "The following code snippet plots the Delta Relative Improvement (DRI) against the Effect Ratio (ER). Having runs with different parametrizations at hand, we can compare them in the cartesian plane. As a rule of thumb, we can say the closer a point to (ER 1, DRI 0), the better the replication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFvR5NAIVtED"
      },
      "source": [
        "rpl_eval = RplEvaluator(qrel_orig_path=QREL,\n",
        "                        run_b_orig_path=ORIG_B,\n",
        "                        run_a_orig_path=ORIG_A,\n",
        "                        run_b_rep_path=None,\n",
        "                        run_a_rep_path=None,\n",
        "                        qrel_rpl_path=QREL_RPL)\n",
        "\n",
        "rpl_eval.trim()\n",
        "rpl_eval.evaluate()\n",
        "\n",
        "for run_name, info in runs_rpl.items():\n",
        "    with open(info.get('path')) as run_file:\n",
        "        info['run'] = pytrec_eval.parse_run(run_file)\n",
        "        trim(info['run'])\n",
        "        info['scores'] = rpl_eval.evaluate(info['run'])\n",
        "\n",
        "dri_er = {\n",
        "    'wcr_tf_1': {\n",
        "        'er': rpl_eval.er(runs_rpl['rpl_wcr04_tf_1']['scores'], runs_rpl['rpl_wcr0405_tf_1']['scores']),\n",
        "        'dri': rpl_eval.dri(runs_rpl['rpl_wcr04_tf_1']['scores'], runs_rpl['rpl_wcr0405_tf_1']['scores'])\n",
        "    },\n",
        "    'wcr_tf_2': {\n",
        "        'er': rpl_eval.er(runs_rpl['rpl_wcr04_tf_2']['scores'], runs_rpl['rpl_wcr0405_tf_2']['scores']),\n",
        "        'dri': rpl_eval.dri(runs_rpl['rpl_wcr04_tf_2']['scores'], runs_rpl['rpl_wcr0405_tf_2']['scores'])\n",
        "    },\n",
        "    'wcr_tf_3': {\n",
        "        'er': rpl_eval.er(runs_rpl['rpl_wcr04_tf_3']['scores'], runs_rpl['rpl_wcr0405_tf_3']['scores']),\n",
        "        'dri': rpl_eval.dri(runs_rpl['rpl_wcr04_tf_3']['scores'], runs_rpl['rpl_wcr0405_tf_3']['scores'])\n",
        "    },\n",
        "    'wcr_tf_4': {\n",
        "        'er': rpl_eval.er(runs_rpl['rpl_wcr04_tf_4']['scores'], runs_rpl['rpl_wcr0405_tf_4']['scores']),\n",
        "        'dri': rpl_eval.dri(runs_rpl['rpl_wcr04_tf_4']['scores'], runs_rpl['rpl_wcr0405_tf_4']['scores'])\n",
        "    },\n",
        "    'wcr_tf_5': {\n",
        "        'er': rpl_eval.er(runs_rpl['rpl_wcr04_tf_5']['scores'], runs_rpl['rpl_wcr0405_tf_5']['scores']),\n",
        "        'dri': rpl_eval.dri(runs_rpl['rpl_wcr04_tf_5']['scores'], runs_rpl['rpl_wcr0405_tf_5']['scores'])\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "measures = ['P_10', 'map', 'ndcg']\n",
        "marker_color = [('o', 'b'), ('^', 'g'), ('v', 'r')]\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
        "ax1.set_xlabel('Effect Ratio (ER)')\n",
        "ax1.set_ylabel(u'Delta Relative Improvement (ΔRI)')\n",
        "\n",
        "for measure, mk in zip(measures, marker_color):\n",
        "    ax1.plot([dri_er[r]['er'][measure] for r in dri_er.keys()],\n",
        "             [dri_er[r]['dri'][measure] for r in dri_er.keys()],\n",
        "             marker=mk[0], color=mk[1], linestyle='None', label=measure)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='k')\n",
        "fig.tight_layout()\n",
        "plt.axhline(0, color='grey')\n",
        "plt.axvline(1, color='grey')\n",
        "plt.legend()\n",
        "plt.title('Replicability')\n",
        "\n",
        "for m in measures:\n",
        "  for r in dri_er.keys():\n",
        "    plt.text(x = dri_er[r]['er'][m], \n",
        "             y = dri_er[r]['dri'][m],\n",
        "             s = r) \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
